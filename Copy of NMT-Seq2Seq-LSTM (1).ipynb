{"cells":[{"cell_type":"markdown","metadata":{"id":"PSuJ-9X_qk1b"},"source":["# Necessary Imports "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"executionInfo":{"elapsed":10168,"status":"ok","timestamp":1651749652051,"user":{"displayName":"14 Aventika khemani","userId":"17647957484136103542"},"user_tz":-330},"id":"S-Ycz13hbUbC","outputId":"2d137cbb-0258-48e8-9614-b734a3d79a5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 64 kB 1.8 MB/s \n","\u001b[K     |████████████████████████████████| 1.2 MB 11.3 MB/s \n","\u001b[?25h"]},{"output_type":"execute_result","data":{"text/plain":["'\\n# Seeding for reproducible results everytime\\nSEED = 777\\n\\nrandom.seed(SEED)\\nnp.random.seed(SEED)\\ntorch.manual_seed(SEED)\\ntorch.cuda.manual_seed(SEED)\\ntorch.backends.cudnn.deterministic = True'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}],"source":["!pip install torchtext==0.6.0 --quiet\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchtext.data import Field, BucketIterator, TabularDataset\n","import numpy as np\n","import pandas as pd\n","import spacy\n","import random\n","from torchtext.data.metrics import bleu_score\n","from pprint import pprint\n","from torch.utils.tensorboard import SummaryWriter\n","from torchsummary import summary\n","'''\n","# Seeding for reproducible results everytime\n","SEED = 777\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True'''"]},{"cell_type":"markdown","metadata":{"id":"42fLcaN_kPxf"},"source":["# 2. Data Preparation & Pre-processing"]},{"cell_type":"markdown","metadata":{"id":"-FHjZ6RnqoNJ"},"source":["Loading the SpaCy's vocabulary for our desired languages. SpaCy also supports many languages like french, german etc,.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7333,"status":"ok","timestamp":1651749659370,"user":{"displayName":"14 Aventika khemani","userId":"17647957484136103542"},"user_tz":-330},"id":"GrNraUABrDq2","outputId":"2e35af31-0541-4974-d05e-234192ad381e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 12.0 MB 5.2 MB/s \n","\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","\u001b[38;5;2m✔ Linking successful\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n","/usr/local/lib/python3.7/dist-packages/spacy/data/en\n","You can now load the model via spacy.load('en')\n"]}],"source":["!python -m spacy download en --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7Da1d8Pb-p4"},"outputs":[],"source":["spacy_english = spacy.load(\"en\")"]},{"cell_type":"markdown","metadata":{"id":"Xu2SLNiZrd0Q"},"source":["Now let's create custom tokenization methods for the languages. Tokenization is a process of breaking the sentence into a list of individual tokens (words).\n","\n","We can make use of PyTorch's TorchText library for data pre-processing and SpaCy for vocabulary building (English and German) & tokenization of our data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":754,"status":"ok","timestamp":1651750384424,"user":{"displayName":"14 Aventika khemani","userId":"17647957484136103542"},"user_tz":-330},"id":"leVROD_6qz16","outputId":"89e0c51a-8ecc-4136-8f6c-6aacb66e6bb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["['are', 'you', 'free', 'today']\n","['YOU', 'FREE', 'TODAY']\n"]}],"source":["def tokenize_english(text):\n","  return [token.text for token in spacy_english.tokenizer(text)]\n","\n","### Sample Run ###\n","\n","sample_text_1 = \"are you free today\"\n","sample_text_2 = \"YOU FREE TODAY\"\n","print(tokenize_english(sample_text_1))\n","print(tokenize_english(sample_text_2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-jJRdqc2-SoW"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"markdown","metadata":{"id":"fRa1BNkOU3nK"},"source":["So once we get to understand what can be done in torch text, let's talk about how it can be implemented in the torch text module. Here we are going to make use of 3 classes under torch text.\n","\n","1. Fields :\n","> This is a class under the torch text, where we specify how the preprocessing should be done on our data corpus.\n","2. TabularDataset : \n","> Using this class, we can actually define the Dataset of columns stored in CSV, TSV, or JSON format and also map them into integers.\n","3. BucketIterator :\n","> Using this class, we can perform padding our data for approximation and make batches with our data for model training.\n","\n","Here our source language (SRC - Input) is German and target language (TRG - Output) is English. We also add 2 extra tokens \"start of sequence\" <sos> and \"end of sequence\" <EOS> for effective model training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yn8CDZ1ssIju"},"outputs":[],"source":["src = Field(tokenize=tokenize_english,\n","               lower=True,\n","               init_token=\"<sos>\",\n","               eos_token=\"<eos>\")\n","\n","trg = Field(tokenize=tokenize_english,\n","               lower=True,\n","               init_token=\"<sos>\",\n","               eos_token=\"<eos>\")\n","\n","fields = {'English': ('src', src), 'ISL-Gloss': ('trg', trg)}\n","\n","train_data, valid_data, test_data = TabularDataset.splits(path = '/content/drive/My Drive/Colab Notebooks/project',\n","                                                          train = '/content/drive/My Drive/Colab Notebooks/project/data_train.csv',\n","                                                          test = '/content/drive/My Drive/Colab Notebooks/project/data_test.csv',\n","                                                          validation = '/content/drive/My Drive/Colab Notebooks/project/data_validate.csv',\n","                                                          format ='csv', \n","                                                          fields = fields )\n","\n","\n","src.build_vocab(train_data, max_size=10000, min_freq=2)\n","trg.build_vocab(train_data, max_size=10000, min_freq=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ILXUMSRVLhb-"},"outputs":[],"source":["print(f\"Unique tokens in source english vocabulary: {len(src.vocab)}\")\n","print(f\"Unique tokens in target isl-gloss vocabulary: {len(trg.vocab)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yiRsZjvEME18"},"outputs":[],"source":["# dir(english.vocab)\n","\n","print(src.vocab.__dict__.keys())\n","print(list(src.vocab.__dict__.values()))\n","e = list(src.vocab.__dict__.values())\n","for i in e:\n","  print(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IhhJy36TM4SV"},"outputs":[],"source":["word_2_idx = dict(e[3])\n","idx_2_word = {}\n","for k,v in word_2_idx.items():\n","  idx_2_word[v] = k"]},{"cell_type":"markdown","metadata":{"id":"Xb-ecGHHxQCS"},"source":["# Dataset sneek peek"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yvt8AUrWvbA_"},"outputs":[],"source":["print(f\"Number of training examples: {len(train_data.examples)}\")\n","print(f\"Number of validation examples: {len(valid_data.examples)}\")\n","print(f\"Number of testing examples: {len(test_data.examples)}\")\n","\n","print(train_data[5].__dict__.keys())\n","pprint(train_data[5].__dict__.values())"]},{"cell_type":"markdown","metadata":{"id":"A5CbOTA-nCMF"},"source":["After setting the language pre-processing criteria, the next step is to create batches of training, testing and validation data using iterators.\n","\n","Creating batches is an exhaustive process, luckily we can make use of TorchText's iterator libraries.\n","\n","Here we are using BucketIterator for effective padding of source and target sentences. We can access the source (german) batch of data using .src attribute and it's correspondign (english) batch of data using .trg attribute."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Gmz5adIwbwF"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","BATCH_SIZE = 4\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), \n","                                                                      batch_size = BATCH_SIZE, \n","                                                                      sort_within_batch=True,\n","                                                                      sort_key=lambda x: len(x.src),\n","                                                                      device = device)"]},{"cell_type":"markdown","metadata":{"id":"xvYIL8X1-LAP"},"source":["## Actual text data before tokenized"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4vWNHTlL8nSg"},"outputs":[],"source":["count = 0\n","max_len_eng = []\n","max_len_isl = []\n","for data in train_data:\n","  max_len_eng.append(len(data.src))\n","  max_len_isl.append(len(data.trg))\n","  if count < 10 :\n","    print(\"English - \",*data.src, \" Length - \", len(data.src))\n","    print(\"ISL Gloss - \",*data.trg, \" Length - \", len(data.trg))\n","    print()\n","  count += 1\n","\n","print(\"Maximum Length of English sentence {} and ISL-Gloss sentence {} in the dataset\".format(max(max_len_eng),max(max_len_isl)))\n","print(\"Minimum Length of English sentence {} and ISL-Gloss sentence {} in the dataset\".format(min(max_len_eng),min(max_len_isl)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYL8BmZI0Bzh"},"outputs":[],"source":["count = 0\n","for data in train_iterator:\n","  if count < 1 :\n","    print(\"Shapes\", data.src.shape, data.trg.shape)\n","    print()\n","    print(\"English - \",*data.src, \" Length - \", len(data.src))\n","    print()\n","    print(\"ISL Gloss - \",*data.trg, \" Length - \", len(data.trg))\n","    temp_eng = data.src\n","    temp_isl = data.trg\n","    count += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kr7ue9mDRNLp"},"outputs":[],"source":["temp_eng_idx = (temp_eng).cpu().detach().numpy()\n","temp_isl_idx = (temp_isl).cpu().detach().numpy()"]},{"cell_type":"markdown","metadata":{"id":"27RquKagotom"},"source":["I just experimented with a batch size of 32 and a sample target batch is shown below. The sentences are tokenized into list of words and indexed according to the vocabulary. The \"pad\" token gets an index of 1.\n","\n","Each column corresponds to a sentence indexed into numbers and we have 32 such sentences in a single target batch and the number of rows corresponds to the maximum length of that sentence. Short sentences are padded with 1 to compensate.\n","The table (Idx.csv) contains the numerical indices of the words, which is later fed into the word embedding and converted into dense representation for Seq2Seq processing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dg028v3Ru7c"},"outputs":[],"source":["df_isl_idx = pd.DataFrame(data = temp_isl_idx, columns = [str(\"S_\")+str(x) for x in np.arange(1, 5)])\n","df_isl_idx.index.name = 'Time Steps'\n","df_isl_idx.index = df_isl_idx.index + 1 \n","# df_isl_idx.to_csv('/content/idx.csv')\n","df_isl_idx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HtkVuHkaT7ba"},"outputs":[],"source":["df_isl_word = pd.DataFrame(columns = [str(\"S_\")+str(x) for x in np.arange(1, 3)])\n","df_isl_word  = df_isl_idx.replace(idx_2_word)\n","# df_isl_word .to_csv('/content/Words.csv')\n","df_isl_word "]},{"cell_type":"markdown","metadata":{"id":"N0sxi0e0lkvK"},"source":["# 3. Long Short Term Memory (LSTM) - Under the Hood"]},{"cell_type":"markdown","metadata":{"id":"7ef7R9ZGy7Ca"},"source":["# 4. Encoder Model Architecture (Seq2Seq)"]},{"cell_type":"markdown","metadata":{"id":"Cbfs74GwmKcq"},"source":["# 5. Encoder Code Implementation (Seq2Seq)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vlOtY31y40q"},"outputs":[],"source":["class EncoderLSTM(nn.Module):\n","  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n","    super(EncoderLSTM, self).__init__()\n","\n","    # Size of the one hot vectors that will be the input to the encoder\n","    #self.input_size = input_size\n","\n","    # Output size of the word embedding NN\n","    #self.embedding_size = embedding_size\n","\n","    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n","    self.hidden_size = hidden_size\n","\n","    # Number of layers in the lstm\n","    self.num_layers = num_layers\n","\n","    # Regularization parameter\n","    self.dropout = nn.Dropout(p)\n","    self.tag = True\n","\n","    # Shape --------------------> (5376, 300) [input size, embedding dims]\n","    self.embedding = nn.Embedding(input_size, embedding_size)\n","    \n","    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n","    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n","\n","  # Shape of x (26, 32) [Sequence_length, batch_size]\n","  def forward(self, x):\n","\n","    # Shape -----------> (26, 32, 300) [Sequence_length , batch_size , embedding dims]\n","    embedding = self.dropout(self.embedding(x))\n","    \n","    # Shape --> outputs (26, 32, 1024) [Sequence_length , batch_size , hidden_size]\n","    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size]\n","    outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n","\n","    return hidden_state, cell_state\n","\n","input_size_encoder = len(src.vocab)\n","encoder_embedding_size = 100\n","hidden_size = 1024\n","num_layers = 2\n","encoder_dropout = 0.5\n","\n","encoder_lstm = EncoderLSTM(input_size_encoder, encoder_embedding_size,\n","                           hidden_size, num_layers, encoder_dropout).to(device)\n","print(encoder_lstm)"]},{"cell_type":"markdown","metadata":{"id":"UtjW6kjByunW"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"cIhD2-OBmVnS"},"source":["# 7. Decoder Code Implementation (Seq2Seq)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnGwwU6p2Zfh"},"outputs":[],"source":["class DecoderLSTM(nn.Module):\n","  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):\n","    super(DecoderLSTM, self).__init__()\n","\n","    # Size of the one hot vectors that will be the input to the encoder\n","    #self.input_size = input_size\n","\n","    # Output size of the word embedding NN\n","    #self.embedding_size = embedding_size\n","\n","    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n","    self.hidden_size = hidden_size\n","\n","    # Number of layers in the lstm\n","    self.num_layers = num_layers\n","\n","    # Size of the one hot vectors that will be the output to the encoder (English Vocab Size)\n","    self.output_size = output_size\n","\n","    # Regularization parameter\n","    self.dropout = nn.Dropout(p)\n","\n","    # Shape --------------------> (5376, 300) [input size, embedding dims]\n","    self.embedding = nn.Embedding(input_size, embedding_size)\n","\n","    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n","    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n","\n","    # Shape -----------> (1024, 4556) [embedding dims, hidden size, num layers]\n","    self.fc = nn.Linear(hidden_size, output_size)\n","\n","  # Shape of x (32) [batch_size]\n","  def forward(self, x, hidden_state, cell_state):\n","\n","    # Shape of x (1, 32) [1, batch_size]\n","    x = x.unsqueeze(0)\n","\n","    # Shape -----------> (1, 32, 300) [1, batch_size, embedding dims]\n","    embedding = self.dropout(self.embedding(x))\n","\n","    # Shape --> outputs (1, 32, 1024) [1, batch_size , hidden_size]\n","    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size] (passing encoder's hs, cs - context vectors)\n","    outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n","\n","    # Shape --> predictions (1, 32, 4556) [ 1, batch_size , output_size]\n","    predictions = self.fc(outputs)\n","\n","    # Shape --> predictions (32, 4556) [batch_size , output_size]\n","    predictions = predictions.squeeze(0)\n","\n","    return predictions, hidden_state, cell_state\n","\n","input_size_decoder = len(trg.vocab)\n","decoder_embedding_size = 100\n","hidden_size = 1024\n","num_layers = 2\n","decoder_dropout = 0.5\n","output_size = len(trg.vocab)\n","\n","decoder_lstm = DecoderLSTM(input_size_decoder, decoder_embedding_size,\n","                           hidden_size, num_layers, decoder_dropout, output_size).to(device)\n","print(decoder_lstm)"]},{"cell_type":"markdown","metadata":{"id":"Sok8t_j76Ozp"},"source":["# 8. Seq2Seq (Encoder + Decoder) Interface"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0rHNbZe7ALr"},"outputs":[],"source":["for batch in train_iterator:\n","  print(batch.src.shape)\n","  print(batch.trg.shape)\n","  break\n","\n","x = batch.trg[1]\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"3al--sEzmfmU"},"source":["# 9. Seq2Seq (Encoder + Decoder) Code Implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UuHGodQe4r9v"},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","  def __init__(self, Encoder_LSTM, Decoder_LSTM):\n","    super(Seq2Seq, self).__init__()\n","    self.Encoder_LSTM = Encoder_LSTM\n","    self.Decoder_LSTM = Decoder_LSTM\n","\n","  def forward(self, source, target, tfr=0.5):\n","    # Shape - Source : (10, 32) [(Sentence length German + some padding), Number of Sentences]\n","    batch_size = source.shape[1]\n","\n","    # Shape - Source : (14, 32) [(Sentence length English + some padding), Number of Sentences]\n","    target_len = target.shape[0]\n","    target_vocab_size = len(trg.vocab)\n","    \n","    # Shape --> outputs (14, 32, 5766) \n","    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n","\n","    # Shape --> (hs, cs) (2, 32, 1024) ,(2, 32, 1024) [num_layers, batch_size size, hidden_size] (contains encoder's hs, cs - context vectors)\n","    hidden_state, cell_state = self.Encoder_LSTM(source)\n","\n","    # Shape of x (32 elements)\n","    x = target[0] # Trigger token <SOS>\n","\n","    for i in range(1, target_len):\n","      # Shape --> output (32, 5766) \n","      output, hidden_state, cell_state = self.Decoder_LSTM(x, hidden_state, cell_state)\n","      outputs[i] = output\n","      best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n","      x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n","\n","    # Shape --> outputs (14, 32, 5766) \n","    return outputs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mOQL9vk49H2U"},"outputs":[],"source":["# Hyperparameters\n","\n","learning_rate = 0.001\n","writer = SummaryWriter(f\"runs/loss_plot\")\n","step = 0\n","\n","model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","pad_idx = trg.vocab.stoi[\"<pad>\"]\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OpsjQCsZ_srZ"},"outputs":[],"source":["model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UtH0Bnq3qFmd"},"outputs":[],"source":["def translate_sentence(model, sentence, german, english, device, max_length=50):\n","    spacy_isl = spacy.load(\"en\")\n","\n","    if type(sentence) == str:\n","        tokens = [token.text.lower() for token in spacy_isl(sentence)]\n","    else:\n","        tokens = [token.lower() for token in sentence]\n","    tokens.insert(0, german.init_token)\n","    tokens.append(german.eos_token)\n","    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n","    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n","\n","    # Build encoder hidden, cell state\n","    with torch.no_grad():\n","        hidden, cell = model.Encoder_LSTM(sentence_tensor)\n","\n","    outputs = [english.vocab.stoi[\"<sos>\"]]\n","\n","    for _ in range(max_length):\n","        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n","\n","        with torch.no_grad():\n","            output, hidden, cell = model.Decoder_LSTM(previous_word, hidden, cell)\n","            best_guess = output.argmax(1).item()\n","\n","        outputs.append(best_guess)\n","\n","        # Model predicts it's the end of the sentence\n","        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n","            break\n","\n","    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n","    return translated_sentence[1:]\n","\n","def bleu(data, model, german, english, device):\n","    targets = []\n","    outputs = []\n","\n","    for example in data:\n","        src = vars(example)[\"src\"]\n","        trg = vars(example)[\"trg\"]\n","\n","        prediction = translate_sentence(model, src, german, english, device)\n","        prediction = prediction[:-1]  # remove <eos> token\n","\n","        targets.append([trg])\n","        outputs.append(prediction)\n","\n","    return bleu_score(outputs, targets)\n","\n","def checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss):\n","    print('saving')\n","    print()\n","    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n","    torch.save(state, '/content/checkpoint-NMT')\n","    torch.save(model.state_dict(),'/content/checkpoint-NMT-SD')"]},{"cell_type":"markdown","metadata":{"id":"W6VnFyCnNlTz"},"source":["# 10. Seq2Seq Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"T4jLBPRD9osT"},"outputs":[],"source":["epoch_loss = 0.0\n","num_epochs = 100\n","best_loss = 999999\n","best_epoch = -1\n","sentence1 = \"I am sleeping.\"\n","ts1  = []\n","\n","for epoch in range(num_epochs):\n","  print(\"Epoch - {} / {}\".format(epoch+1, num_epochs))\n","  model.eval()\n","  translated_sentence1 = translate_sentence(model, sentence1, src, trg, device, max_length=50)\n","  print(f\"Translated example sentence 1: \\n {translated_sentence1}\")\n","  ts1.append(translated_sentence1)\n","\n","  model.train(True)\n","  for batch_idx, batch in enumerate(train_iterator):\n","    input = batch.src.to(device)\n","    target = batch.trg.to(device)\n","\n","    # Pass the input and target for model's forward method\n","    output = model(input, target)\n","    output = output[1:].reshape(-1, output.shape[2])\n","    target = target[1:].reshape(-1)\n","\n","    # Clear the accumulating gradients\n","    optimizer.zero_grad()\n","\n","    # Calculate the loss value for every epoch\n","    loss = criterion(output, target)\n","\n","    # Calculate the gradients for weights & biases using back-propagation\n","    loss.backward()\n","\n","    # Clip the gradient value is it exceeds > 1\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","\n","    # Update the weights values using the gradients we calculated using bp \n","    optimizer.step()\n","    step += 1\n","    epoch_loss += loss.item()\n","    writer.add_scalar(\"Training loss\", loss, global_step=step)\n","\n","  if epoch_loss < best_loss:\n","    best_loss = epoch_loss\n","    best_epoch = epoch\n","    checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss) \n","    if ((epoch - best_epoch) >= 10):\n","      print(\"no improvement in 10 epochs, break\")\n","      break\n","  print(\"Epoch_Loss - {}\".format(loss.item()))\n","  print()\n","  \n","print(epoch_loss / len(train_iterator))\n","\n","score = bleu(test_data[1:100], model, src, trg, device)\n","print(f\"Bleu score {score*100:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VSrFmvJhBNIA"},"outputs":[],"source":["score = bleu(test_data, model, src, trg, device)\n","print(f\"Bleu score {score}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AqylUksMJtVo"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir runs/"]},{"cell_type":"markdown","metadata":{"id":"-yyYsPQ2ml7Y"},"source":["# 11. Seq2Seq Model Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s8seg8haidFT"},"outputs":[],"source":["progress  = []\n","import nltk\n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","for i, sen in enumerate(ts1):\n","  progress.append(TreebankWordDetokenizer().detokenize(sen))\n","print(progress)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VH2U-LbhH1dw"},"outputs":[],"source":["progress_df = pd.DataFrame(data = progress, columns=['Predicted Sentence'])\n","progress_df.index.name = \"Epochs\"\n","progress_df.to_csv('/content/predicted_sentence.csv')\n","progress_df.head()"]},{"cell_type":"markdown","metadata":{"id":"TAUieohaNpvd"},"source":["# Model Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BxcYB6cRJKIZ"},"outputs":[],"source":["model.eval()\n","test_sentences  = [\"He stuck the broken pieces together.\", \"The food is getting cold.\", \"I cooked dinner.\"]\n","actual_sentences  = [\"he broken piece stick broken\", \"food cold get\", \"i dinner cook\"]\n","pred_sentences = []\n","\n","for idx, i in enumerate(test_sentences):\n","  model.eval()\n","  translated_sentence = translate_sentence(model, i, src, trg, device, max_length=50)\n","  progress.append(TreebankWordDetokenizer().detokenize(translated_sentence))\n","  print(\"English : {}\".format(i))\n","  print(\"Actual Sentence in ISL Gloss : {}\".format(actual_sentences[idx]))\n","  print(\"Predicted Sentence in ISL Gloss : {}\".format(progress[-1]))\n","  print()\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of NMT-Seq2Seq-LSTM.ipynb","provenance":[{"file_id":"16edJ5V2083Zqx9Q4c5mktByYQ-RF9X-i","timestamp":1649396899699},{"file_id":"https://github.com/bala-codes/Natural-Language-Processing-NLP/blob/master/Neural%20Machine%20Translation/1.%20Seq2Seq%20%5BEnc%20%2B%20Dec%5D%20Model%20for%20Neural%20Machine%20Translation%20(Without%20Attention%20Mechanism).ipynb","timestamp":1649265949654}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}